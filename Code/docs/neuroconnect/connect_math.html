<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>neuroconnect.connect_math API documentation</title>
<meta name="description" content="Basic maths functions related to graph connection probability." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neuroconnect.connect_math</code></h1>
</header>
<section id="section-intro">
<p>Basic maths functions related to graph connection probability.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Basic maths functions related to graph connection probability.&#34;&#34;&#34;
from math import ceil, floor, sqrt, isclose
from scipy import sparse, stats, interpolate
from collections import OrderedDict

import mpmath
import numpy as np

from .plot import plot_acc_interp


def C(n, r):
    &#34;&#34;&#34;Implement n choose r, works on non integers via gamma.&#34;&#34;&#34;
    return mpmath.binomial(n, r)


def binomial_pmf(n, k, p):
    &#34;&#34;&#34;Binomial: n draws, k successes, and p proportion successes.&#34;&#34;&#34;
    bin_res = C(n, k) * mpmath.power(p, k) * mpmath.power(1 - p, n - k)
    return bin_res


def hypergeometric_pmf(N, K, n, k, approx=False):
    &#34;&#34;&#34;
    Calculates the hypergeometric probability mass function.

    See https://en.wikipedia.org/wiki/Hypergeometric_distribution

    Parameters
    ----------
    N : float
        The population size.
    K : float
        The number of success states in the population.
    n : int
        The number of draws.
    k : int
        The number of observed successes.
    approx : bool
        If True, approximate the result by the binomial distribution.

    Returns
    -------
    float
        The pmf pX(k).

    &#34;&#34;&#34;
    if approx is True:
        proportion_true = K / N
        return binomial_pmf(n, k, proportion_true)

    left = C(K, k)
    right = C(N - K, n - k)
    btm = C(N, n)
    return left * right / btm


def expected_unique(N, k, do_round=True):
    &#34;&#34;&#34;
    Calculates the expected number of unique values when drawing k from N.

    See https://math.stackexchange.com/questions/72223/finding-expected-number-of-distinct-values-selected-from-a-set-of-integers

    Parameters
    ----------
    N : int
        The population size.
    k : int
        The number of draws.

    Returns
    -------
    float
        The expected number of distinct values.

    &#34;&#34;&#34;
    n = mpmath.mpf(N)
    p = mpmath.mpf(k)
    temp = mpmath.power((n - 1) / n, p)
    if do_round:
        return max(int(round(n * (1 - temp))), 0)
    else:
        return max(float(n * (1 - temp)), 0)


def expected_overlapping(total, good, draws):
    &#34;&#34;&#34;Calculates the expected overlapping draws from total of good.&#34;&#34;&#34;
    if total == 0:
        return 0
    drawn = int(round(draws * (good / total)))
    return min(max(drawn, 0), int(round(good)))


def expected_non_overlapping(total, bad, draws):
    &#34;&#34;&#34;Calculates the expected non-overlapping draws from total of bad.&#34;&#34;&#34;
    good = total - bad
    return expected_overlapping(total, good, draws)


def window_2d_avg(matrix, k_size):
    &#34;&#34;&#34;Get the average of matrix in k_size * k_size windows.&#34;&#34;&#34;
    x, y = matrix.shape
    add_val = 1 / (k_size * k_size)

    new_x = int(ceil(x / k_size))
    new_y = int(ceil(y / k_size))

    out = np.zeros(shape=(new_x, new_y), dtype=np.float32)

    nonzeros_x, nonzeros_y, _ = sparse.find(matrix)
    for i, j in zip(nonzeros_x, nonzeros_y):
        x_idx = int(floor(i / k_size))
        y_idx = int(floor(j / k_size))
        out[x_idx, y_idx] += add_val

    # for i in range(new_x):
    #     for j in range(new_y):
    #         out[i, j] = np.mean(
    #             matrix[i * k_size : (i + 1) * k_size, j * k_size : (j + 1) * k_size]
    #         )

    return out


def multi_2d_avg(AB, BA, AA, BB, k_size):
    &#34;&#34;&#34;Combine separate into one sparse matrix.&#34;&#34;&#34;
    x, y = AB.shape

    a_size = int(ceil(x / k_size))
    b_size = int(ceil(y / k_size))

    full_mat = np.zeros(shape=(a_size + b_size, a_size + b_size), dtype=np.float32)
    full_mat[:a_size, :a_size] = window_2d_avg(AA, k_size)
    full_mat[:a_size, a_size:] = window_2d_avg(AB, k_size)
    full_mat[a_size:, :a_size] = window_2d_avg(BA, k_size)
    full_mat[a_size:, a_size:] = window_2d_avg(BB, k_size)

    return full_mat


def convolution(X, Y, sub=None):
    &#34;&#34;&#34;Computes Z = X + Y as distributions.&#34;&#34;&#34;
    min_val = np.array(list(X.keys())).min() + np.array(list(Y.keys())).min()
    max_val = np.array(list(X.keys())).max() + np.array(list(Y.keys())).max()

    out_dict = OrderedDict()
    r1 = range(min_val, max_val + 1)
    first_iter, s1 = subsample_list(r1, sub)
    for i in first_iter:
        out_dict[i] = 0
        r2 = range(0, i + 1)
        if len(r1) &gt; 1:
            second_iter, s2 = subsample_list(r2, sub)
        else:
            second_iter, s2 = r2, False
        values = np.zeros(len(second_iter))
        for j_idx, j in enumerate(second_iter):
            val = X.get(j, 0) * Y.get(i - j, 0)
            values[j_idx] = val
        if s2:
            values = interp(list(r2), second_iter, values)
        out_dict[i] = np.sum(values)

    if s1:
        keys = list(r1)
        final_vals = interp(keys, list(out_dict.keys()), list(out_dict.values()))
        total = np.sum(final_vals)
        out_dict = OrderedDict()
        for k, v in zip(keys, final_vals):
            out_dict[k] = v / total

    return out_dict


def nfold_conv(args, sub=None):
    &#34;&#34;&#34;Computes SUM(args) where args are distributions.&#34;&#34;&#34;
    if len(args) &gt;= 2:
        conv = convolution(args[0], args[1], sub=sub)
        for arg in args[2:]:
            conv = convolution(conv, arg, sub=sub)
        return conv
    else:
        return args[0]


def create_uniform(min_val, max_val):
    &#34;&#34;&#34;Create a uniform distribution with min and max values.&#34;&#34;&#34;
    dist = OrderedDict()

    div = 1 / (max_val + 1 - min_val)
    for val in range(min_val, max_val + 1):
        dist[val] = div

    return dist


def get_dist_mean(dist):
    &#34;&#34;&#34;Get the expected value of a distribution.&#34;&#34;&#34;
    mean = 0
    for k, v in dist.items():
        mean += k * v

    return mean


def get_dist_var(dist):
    &#34;&#34;&#34;Get the variance of a distribution (OrderedDict).&#34;&#34;&#34;
    mean = get_dist_mean(dist)

    var = 0
    for k, v in dist.items():
        to_add = v * (k - mean) * (k - mean)
        var += to_add

    return var


def trim_zeros(dist):
    new_dist = OrderedDict()

    values = []
    for k, v in dist.items():
        if v != 0.0:
            values.append(k)

    if len(values) &gt; 1:
        min_val = min(values)
        max_val = max(values)

        for i in range(min_val, max_val + 1):
            new_dist[i] = dist.get(i, 0)
    else:
        new_dist[values[0]] = dist.get(values[0], 0)

    return new_dist


def apply_fn_to_dist(dist, fn, sub=None):
    &#34;&#34;&#34;Apply a function to the x values in a distribution - linearly interpolates.&#34;&#34;&#34;
    new_dist = OrderedDict()

    if sub is not None:
        dist = trim_zeros(dist)
        keys = list(dist.keys())
        if not isinstance(keys[0], int):
            raise ValueError(&#34;Distribution must have integer keys, got {}&#34;.format(keys))
        keys, did_sub = subsample_list(keys, sub)
    else:
        keys = list(dist.keys())

    for k in keys:
        evaled = fn(k)
        v = dist[k]
        if evaled in new_dist:
            new_dist[evaled] += v
        else:
            new_dist[evaled] = v

    return dist_from_samples(new_dist)


def random_draw_dist(
    max_samples, dist, total, apply_fn=True, keep_all=False, clt_start=30, sub=None
):
    &#34;&#34;&#34;Expected unique values with n_samples from dist from total possible.&#34;&#34;&#34;

    def fn_to_apply(k):
        return float(expected_unique(total, k))

    sum_dist = None
    mean, var = get_dist_mean(dist), get_dist_var(dist)
    results = OrderedDict()
    for n_samples in range(0, int(max_samples) + 1):
        if n_samples == 0:
            if keep_all:
                to_out = OrderedDict()
                to_out[0] = 1
                results[n_samples] = to_out
            else:
                results[n_samples] = 0
            continue
        elif n_samples &lt;= 2:
            dists_to_conv = [dist] * int(n_samples)
        else:
            dists_to_conv = [sum_dist, dist]
        if n_samples &lt; clt_start:
            sum_dist = nfold_conv(dists_to_conv, sub=sub)
        else:
            max_val = max(list(dist.keys()))
            sum_dist = create_normal(
                range((int(max_val * n_samples)) + 1),
                mean * n_samples,
                var * n_samples,
                sub=sub,
                interp_after=True,
            )
        if apply_fn is False:
            results[n_samples] = sum_dist
            continue
        fn_dist = apply_fn_to_dist(sum_dist, fn_to_apply)
        if keep_all:
            results[n_samples] = fn_dist
        else:
            results[n_samples] = get_dist_mean(fn_dist)

    return results


def interp(x_samps, xvals, yvals):
    &#34;&#34;&#34;Interpolate linearly between xvals and yvals at x_samps.&#34;&#34;&#34;
    if not np.all(np.diff(xvals) &gt; 0):
        raise ValueError(&#34;Require monotonically increasing x values to interp&#34;)
    try:
        if len(xvals) == 1:
            yinterp = yvals
        kind = &#34;linear&#34;
        f = interpolate.interp1d(xvals, yvals, kind=kind)
        yinterp = f(x_samps)
    except BaseException:
        yinterp = np.interp(x_samps, xvals, yvals)

    return yinterp


def dist_from_samples(dist):
    &#34;&#34;&#34;Perform linear interpolation to estimate a distribution from samples.&#34;&#34;&#34;
    xvals = np.array(list(dist.keys()))
    yvals = np.array(list(dist.values()))
    min_val = int(floor(xvals.min()))
    max_val = int(ceil(xvals.max()))
    x_samps = np.array([i for i in range(min_val, max_val + 1)])
    if not np.all(np.diff(xvals) &gt; 0):
        raise ValueError(&#34;Need monotonically increasing x points, got {}&#34;.format(xvals))

    yinterp = interp(x_samps, xvals, yvals)
    div_val = np.sum(yinterp)
    if div_val == 0:
        raise ValueError(&#34;Got zero probability from x-{}, y-{}&#34;.format(xvals, yvals))
    yinterp = yinterp / div_val

    new_dist = OrderedDict()
    for k, v in zip(x_samps, yinterp):
        new_dist[int(k)] = v

    return new_dist


def alt_way(N, samps, senders, min_val, max_val):
    &#34;&#34;&#34;Alternative calculation of probability connection.&#34;&#34;&#34;
    prob_a_senders = OrderedDict()

    for i in range(samps + 1):
        prob_a_senders[i] = float(hypergeometric_pmf(N, senders, samps, i))

    dist = create_uniform(min_val, max_val)
    dists = random_draw_dist(samps, dist, N, keep_all=True)

    to_eval = [i for i in range(N + 1)]
    weighted_dist = OrderedDict()
    for k in to_eval:
        weighted_dist[k] = 0

    for g in dists.keys():
        dist = dists[g]
        prob = prob_a_senders[g]
        for k in to_eval:
            weighted_dist[k] += prob * dist.get(k, 0)

    weighted_dist_vals = np.array(list(weighted_dist.values()))
    pdf_total = np.sum(weighted_dist_vals)
    if pdf_total - 1.0 &gt; 0.0001:
        raise RuntimeError(&#34;PDF does not sum to 1, got {}.&#34;.format(pdf_total))

    final_res = OrderedDict()

    for i in range(samps + 1):

        def fn_to_apply(k):
            return float(hypergeometric_pmf(N, k, samps, i))

        hyper_weight = 0
        for k, v in weighted_dist.items():
            hyper_weight += fn_to_apply(k) * v
        final_res[i] = hyper_weight

    return weighted_dist, final_res


def second_deriv_improve_interp(x_samps, xvals, yvals, dist, fn_to_eval, x_val):
    &#34;&#34;&#34;Given samples at xvals and yvals, second derivative to detect change points.&#34;&#34;&#34;
    grad = np.gradient(yvals)
    diffs = np.abs(grad)
    grad_2 = np.gradient(grad)

    m = np.mean(diffs)
    s = np.std(diffs)
    ub = m + (3 * s)
    outliers = []
    for i, val in enumerate(diffs[:2]):
        if val &gt;= ub:
            outliers.append(i)
            outliers.append(i + 1)
    for i, val in enumerate(diffs[-2:]):
        if val &gt;= ub:
            outliers.append(i)
            outliers.append(i + 1)

    for i in range(len(grad_2) - 1):
        tc = abs(grad_2[i + 1] - grad_2[i])
        cv = np.abs((grad_2[i + 1] + grad_2[i]))
        if tc &gt; cv and tc &gt; (0.00000000001):
            outliers.append(i)

    outliers = sorted(list(set(outliers)))

    total_so_far = 0
    for val in outliers:
        start_idx = total_so_far + val
        new_x = np.array(
            list(range(xvals[start_idx] + 1, xvals[start_idx + 1])),
            dtype=np.int32,
        )
        new_y = np.zeros(
            shape=(len(new_x)),
            dtype=np.float64,
        )
        for i_idx, i in enumerate(new_x):
            v = dist[i]
            if not isclose(v, 0.0, abs_tol=1e-8):
                new_y[i_idx] = v * fn_to_eval(i, x_val)
            else:
                new_y[i_idx] = 0.0

        first_bit_x, second_bit_x = (
            xvals[: start_idx + 1],
            xvals[start_idx + 1 :],
        )
        first_bit_y, second_bit_y = (
            yvals[: start_idx + 1],
            yvals[start_idx + 1 :],
        )
        total_so_far += len(new_x)
        xvals = np.concatenate((first_bit_x, new_x, second_bit_x))
        yvals = np.concatenate((first_bit_y, new_y, second_bit_y))

    return xvals, yvals


def marginal_prob(
    eval_range,
    dist,
    fn_to_eval,
    x=None,
    sub=None,
    plot=False,
    true_plot=False,
    keep_all=False,
):
    &#34;&#34;&#34;Marginal probability of dist over eval_range calculating fn.&#34;&#34;&#34;
    final_res = OrderedDict()

    def x_prob(x_val):
        if sub is not None:
            in_sub = sub / 10
        else:
            in_sub = sub

        keys, did_sub = subsample_list(dist.keys(), in_sub)
        weighted_prob = np.zeros(shape=len(keys), dtype=np.float64)
        for i, k in enumerate(keys):
            v = dist[k]
            if not isclose(v, 0.0, abs_tol=1e-8):
                weighted_prob[i] = v * fn_to_eval(k, x_val)

        if did_sub:
            x_samps = list(dist.keys())
            xvals = keys
            yvals = weighted_prob
            xvals, yvals = second_deriv_improve_interp(
                x_samps, xvals, yvals, dist, fn_to_eval, x_val
            )
            weighted_prob = interp(x_samps, xvals, yvals)
            weighted_prob[weighted_prob &lt; 0] = 0.0

            if plot is True:
                if true_plot is True:
                    keys, did_sub = subsample_list(dist.keys(), None)
                    true_y = np.zeros(shape=len(keys), dtype=np.float64)
                    for i, k in enumerate(keys):
                        v = dist[k]
                        true_y[i] = v * fn_to_eval(k, x_val)
                else:
                    true_y = None
                plot_acc_interp(
                    x_samps,
                    weighted_prob,
                    xvals,
                    yvals,
                    &#34;dist_marg_{}.pdf&#34;.format(x_val),
                    true_y=true_y,
                )

        if keep_all:
            od = OrderedDict()
            for k, v in zip(dist.keys(), weighted_prob):
                od[k] = v
            return od
        else:
            return np.sum(weighted_prob)

    if x is None:
        for i in eval_range:
            final_res[i] = x_prob(i)
        if keep_all:
            return final_res

        final_res_vals = np.array(list(final_res.values()))
        pdf_total = np.sum(final_res_vals)
        if sub is not None:
            for k, v in final_res.items():
                final_res[k] = v / pdf_total
        else:
            if abs(pdf_total - 1.0) &gt; 0.0001:
                raise RuntimeError(
                    &#34;PDF does not sum to 1, got {} which sums to {}.&#34;.format(
                        final_res_vals, pdf_total
                    )
                )
        return final_res
    else:
        if sub is not None:
            raise ValueError(&#34;x None and sub None together not compatible.&#34;)
        return x_prob(x)


def combine_dists(to_eval, dists_dict, dist, sub=None):
    &#34;&#34;&#34;
    Compute the final weighted distribution for each value in to_eval.

    dist should describe a distribution, which dists_dict depends on
    dists_dict should describe a distribution for each value in dist
    to_eval should be the x-axis in the final distribution

    The function returns the distribution for P(X=x) in to_eval.
    &#34;&#34;&#34;
    weighted_dist = OrderedDict()
    to_eval, did_sub = subsample_list(to_eval, sub)
    for k in to_eval:
        weighted_dist[k] = 0
    for g in dist.keys():
        full_dist = dists_dict[g]
        prob = dist[g]
        for k in to_eval:
            weighted_dist[k] += prob * full_dist.get(k, 0)

    if did_sub:
        weighted_dist = dist_from_samples(weighted_dist)

    pdf_total = np.sum(np.array(list(weighted_dist.values())))
    if abs(pdf_total - 1.0) &gt; 0.0001:
        raise RuntimeError(&#34;PDF does not sum to 1, got {}.&#34;.format(pdf_total))

    return weighted_dist


def create_normal(to_eval, mean, var, sub=None, interp_after=False):
    &#34;&#34;&#34;Create a normal distribution, discretised to a range.&#34;&#34;&#34;
    final_res = OrderedDict()
    if var == 0:
        final_res[int(mean)] = 1.0
        return final_res

    normal = stats.norm(loc=mean, scale=sqrt(var))
    to_eval_t, did_sub = subsample_list(to_eval, sub)
    vals = normal.pdf(to_eval_t)

    if interp_after and did_sub:
        x_samps = list(to_eval)
        xvals = to_eval_t
        yvals = vals

        vals = interp(x_samps, xvals, yvals)

    else:
        to_eval = to_eval_t

    for e, val in zip(to_eval, vals):
        final_res[int(e)] = val

    return final_res


def get_uniform_moments(min_val, max_val):
    &#34;&#34;&#34;Get mean, var of a uniform distribution.&#34;&#34;&#34;
    mean = (max_val + min_val) / 2
    var = (1 / 12) * (max_val - min_val) * (max_val - min_val)

    return mean, var


def subsample_list(list_to_sub, sub_rate):
    &#34;&#34;&#34;Subsample a list that is assumed to have linearly spaced values.&#34;&#34;&#34;
    to_eval_temp = list(list_to_sub)
    if sub_rate is not None:
        min_ev = min(to_eval_temp)
        max_ev = max(to_eval_temp)
        step = int(floor(max_ev - min_ev) * sub_rate)
        if step == 0:
            return to_eval_temp, False
        else:
            to_eval = list(range(min_ev, max_ev, step))
        if max_ev not in to_eval:
            to_eval.append(max_ev)
        return to_eval, True
    else:
        return to_eval_temp, False


def dist_from_file(filename):
    &#34;&#34;&#34;Load a distribution from a file.&#34;&#34;&#34;
    with open(filename, &#34;r&#34;) as f:
        od = OrderedDict()
        lines = f.readlines()
        for line in lines:
            pieces = line.split(&#34;:&#34;)
            k = pieces[0].strip()
            v = float(pieces[1].strip())
            od[k] = v
    summed = np.sum(np.array(list(od.values())))
    if abs(summed - 1.0) &gt; 0.00001:
        raise RuntimeError(&#34;Distribution does not sum to 1, got {}&#34;.format(summed))
    return od


def get_dist_ci(dist):
    &#34;&#34;&#34;Return 95% confidence interval.&#34;&#34;&#34;
    mean = get_dist_mean(dist)
    var = get_dist_var(dist)

    lower = mean - 1.96 * sqrt(var)
    upper = mean + 1.96 * sqrt(var)

    return lower, upper</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="neuroconnect.connect_math.C"><code class="name flex">
<span>def <span class="ident">C</span></span>(<span>n, r)</span>
</code></dt>
<dd>
<div class="desc"><p>Implement n choose r, works on non integers via gamma.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def C(n, r):
    &#34;&#34;&#34;Implement n choose r, works on non integers via gamma.&#34;&#34;&#34;
    return mpmath.binomial(n, r)</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.alt_way"><code class="name flex">
<span>def <span class="ident">alt_way</span></span>(<span>N, samps, senders, min_val, max_val)</span>
</code></dt>
<dd>
<div class="desc"><p>Alternative calculation of probability connection.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alt_way(N, samps, senders, min_val, max_val):
    &#34;&#34;&#34;Alternative calculation of probability connection.&#34;&#34;&#34;
    prob_a_senders = OrderedDict()

    for i in range(samps + 1):
        prob_a_senders[i] = float(hypergeometric_pmf(N, senders, samps, i))

    dist = create_uniform(min_val, max_val)
    dists = random_draw_dist(samps, dist, N, keep_all=True)

    to_eval = [i for i in range(N + 1)]
    weighted_dist = OrderedDict()
    for k in to_eval:
        weighted_dist[k] = 0

    for g in dists.keys():
        dist = dists[g]
        prob = prob_a_senders[g]
        for k in to_eval:
            weighted_dist[k] += prob * dist.get(k, 0)

    weighted_dist_vals = np.array(list(weighted_dist.values()))
    pdf_total = np.sum(weighted_dist_vals)
    if pdf_total - 1.0 &gt; 0.0001:
        raise RuntimeError(&#34;PDF does not sum to 1, got {}.&#34;.format(pdf_total))

    final_res = OrderedDict()

    for i in range(samps + 1):

        def fn_to_apply(k):
            return float(hypergeometric_pmf(N, k, samps, i))

        hyper_weight = 0
        for k, v in weighted_dist.items():
            hyper_weight += fn_to_apply(k) * v
        final_res[i] = hyper_weight

    return weighted_dist, final_res</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.apply_fn_to_dist"><code class="name flex">
<span>def <span class="ident">apply_fn_to_dist</span></span>(<span>dist, fn, sub=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Apply a function to the x values in a distribution - linearly interpolates.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_fn_to_dist(dist, fn, sub=None):
    &#34;&#34;&#34;Apply a function to the x values in a distribution - linearly interpolates.&#34;&#34;&#34;
    new_dist = OrderedDict()

    if sub is not None:
        dist = trim_zeros(dist)
        keys = list(dist.keys())
        if not isinstance(keys[0], int):
            raise ValueError(&#34;Distribution must have integer keys, got {}&#34;.format(keys))
        keys, did_sub = subsample_list(keys, sub)
    else:
        keys = list(dist.keys())

    for k in keys:
        evaled = fn(k)
        v = dist[k]
        if evaled in new_dist:
            new_dist[evaled] += v
        else:
            new_dist[evaled] = v

    return dist_from_samples(new_dist)</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.binomial_pmf"><code class="name flex">
<span>def <span class="ident">binomial_pmf</span></span>(<span>n, k, p)</span>
</code></dt>
<dd>
<div class="desc"><p>Binomial: n draws, k successes, and p proportion successes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def binomial_pmf(n, k, p):
    &#34;&#34;&#34;Binomial: n draws, k successes, and p proportion successes.&#34;&#34;&#34;
    bin_res = C(n, k) * mpmath.power(p, k) * mpmath.power(1 - p, n - k)
    return bin_res</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.combine_dists"><code class="name flex">
<span>def <span class="ident">combine_dists</span></span>(<span>to_eval, dists_dict, dist, sub=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the final weighted distribution for each value in to_eval.</p>
<p>dist should describe a distribution, which dists_dict depends on
dists_dict should describe a distribution for each value in dist
to_eval should be the x-axis in the final distribution</p>
<p>The function returns the distribution for P(X=x) in to_eval.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_dists(to_eval, dists_dict, dist, sub=None):
    &#34;&#34;&#34;
    Compute the final weighted distribution for each value in to_eval.

    dist should describe a distribution, which dists_dict depends on
    dists_dict should describe a distribution for each value in dist
    to_eval should be the x-axis in the final distribution

    The function returns the distribution for P(X=x) in to_eval.
    &#34;&#34;&#34;
    weighted_dist = OrderedDict()
    to_eval, did_sub = subsample_list(to_eval, sub)
    for k in to_eval:
        weighted_dist[k] = 0
    for g in dist.keys():
        full_dist = dists_dict[g]
        prob = dist[g]
        for k in to_eval:
            weighted_dist[k] += prob * full_dist.get(k, 0)

    if did_sub:
        weighted_dist = dist_from_samples(weighted_dist)

    pdf_total = np.sum(np.array(list(weighted_dist.values())))
    if abs(pdf_total - 1.0) &gt; 0.0001:
        raise RuntimeError(&#34;PDF does not sum to 1, got {}.&#34;.format(pdf_total))

    return weighted_dist</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.convolution"><code class="name flex">
<span>def <span class="ident">convolution</span></span>(<span>X, Y, sub=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes Z = X + Y as distributions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolution(X, Y, sub=None):
    &#34;&#34;&#34;Computes Z = X + Y as distributions.&#34;&#34;&#34;
    min_val = np.array(list(X.keys())).min() + np.array(list(Y.keys())).min()
    max_val = np.array(list(X.keys())).max() + np.array(list(Y.keys())).max()

    out_dict = OrderedDict()
    r1 = range(min_val, max_val + 1)
    first_iter, s1 = subsample_list(r1, sub)
    for i in first_iter:
        out_dict[i] = 0
        r2 = range(0, i + 1)
        if len(r1) &gt; 1:
            second_iter, s2 = subsample_list(r2, sub)
        else:
            second_iter, s2 = r2, False
        values = np.zeros(len(second_iter))
        for j_idx, j in enumerate(second_iter):
            val = X.get(j, 0) * Y.get(i - j, 0)
            values[j_idx] = val
        if s2:
            values = interp(list(r2), second_iter, values)
        out_dict[i] = np.sum(values)

    if s1:
        keys = list(r1)
        final_vals = interp(keys, list(out_dict.keys()), list(out_dict.values()))
        total = np.sum(final_vals)
        out_dict = OrderedDict()
        for k, v in zip(keys, final_vals):
            out_dict[k] = v / total

    return out_dict</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.create_normal"><code class="name flex">
<span>def <span class="ident">create_normal</span></span>(<span>to_eval, mean, var, sub=None, interp_after=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a normal distribution, discretised to a range.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_normal(to_eval, mean, var, sub=None, interp_after=False):
    &#34;&#34;&#34;Create a normal distribution, discretised to a range.&#34;&#34;&#34;
    final_res = OrderedDict()
    if var == 0:
        final_res[int(mean)] = 1.0
        return final_res

    normal = stats.norm(loc=mean, scale=sqrt(var))
    to_eval_t, did_sub = subsample_list(to_eval, sub)
    vals = normal.pdf(to_eval_t)

    if interp_after and did_sub:
        x_samps = list(to_eval)
        xvals = to_eval_t
        yvals = vals

        vals = interp(x_samps, xvals, yvals)

    else:
        to_eval = to_eval_t

    for e, val in zip(to_eval, vals):
        final_res[int(e)] = val

    return final_res</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.create_uniform"><code class="name flex">
<span>def <span class="ident">create_uniform</span></span>(<span>min_val, max_val)</span>
</code></dt>
<dd>
<div class="desc"><p>Create a uniform distribution with min and max values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_uniform(min_val, max_val):
    &#34;&#34;&#34;Create a uniform distribution with min and max values.&#34;&#34;&#34;
    dist = OrderedDict()

    div = 1 / (max_val + 1 - min_val)
    for val in range(min_val, max_val + 1):
        dist[val] = div

    return dist</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.dist_from_file"><code class="name flex">
<span>def <span class="ident">dist_from_file</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Load a distribution from a file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dist_from_file(filename):
    &#34;&#34;&#34;Load a distribution from a file.&#34;&#34;&#34;
    with open(filename, &#34;r&#34;) as f:
        od = OrderedDict()
        lines = f.readlines()
        for line in lines:
            pieces = line.split(&#34;:&#34;)
            k = pieces[0].strip()
            v = float(pieces[1].strip())
            od[k] = v
    summed = np.sum(np.array(list(od.values())))
    if abs(summed - 1.0) &gt; 0.00001:
        raise RuntimeError(&#34;Distribution does not sum to 1, got {}&#34;.format(summed))
    return od</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.dist_from_samples"><code class="name flex">
<span>def <span class="ident">dist_from_samples</span></span>(<span>dist)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform linear interpolation to estimate a distribution from samples.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dist_from_samples(dist):
    &#34;&#34;&#34;Perform linear interpolation to estimate a distribution from samples.&#34;&#34;&#34;
    xvals = np.array(list(dist.keys()))
    yvals = np.array(list(dist.values()))
    min_val = int(floor(xvals.min()))
    max_val = int(ceil(xvals.max()))
    x_samps = np.array([i for i in range(min_val, max_val + 1)])
    if not np.all(np.diff(xvals) &gt; 0):
        raise ValueError(&#34;Need monotonically increasing x points, got {}&#34;.format(xvals))

    yinterp = interp(x_samps, xvals, yvals)
    div_val = np.sum(yinterp)
    if div_val == 0:
        raise ValueError(&#34;Got zero probability from x-{}, y-{}&#34;.format(xvals, yvals))
    yinterp = yinterp / div_val

    new_dist = OrderedDict()
    for k, v in zip(x_samps, yinterp):
        new_dist[int(k)] = v

    return new_dist</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.expected_non_overlapping"><code class="name flex">
<span>def <span class="ident">expected_non_overlapping</span></span>(<span>total, bad, draws)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the expected non-overlapping draws from total of bad.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expected_non_overlapping(total, bad, draws):
    &#34;&#34;&#34;Calculates the expected non-overlapping draws from total of bad.&#34;&#34;&#34;
    good = total - bad
    return expected_overlapping(total, good, draws)</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.expected_overlapping"><code class="name flex">
<span>def <span class="ident">expected_overlapping</span></span>(<span>total, good, draws)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the expected overlapping draws from total of good.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expected_overlapping(total, good, draws):
    &#34;&#34;&#34;Calculates the expected overlapping draws from total of good.&#34;&#34;&#34;
    if total == 0:
        return 0
    drawn = int(round(draws * (good / total)))
    return min(max(drawn, 0), int(round(good)))</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.expected_unique"><code class="name flex">
<span>def <span class="ident">expected_unique</span></span>(<span>N, k, do_round=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the expected number of unique values when drawing k from N.</p>
<p>See <a href="https://math.stackexchange.com/questions/72223/finding-expected-number-of-distinct-values-selected-from-a-set-of-integers">https://math.stackexchange.com/questions/72223/finding-expected-number-of-distinct-values-selected-from-a-set-of-integers</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>The population size.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of draws.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The expected number of distinct values.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expected_unique(N, k, do_round=True):
    &#34;&#34;&#34;
    Calculates the expected number of unique values when drawing k from N.

    See https://math.stackexchange.com/questions/72223/finding-expected-number-of-distinct-values-selected-from-a-set-of-integers

    Parameters
    ----------
    N : int
        The population size.
    k : int
        The number of draws.

    Returns
    -------
    float
        The expected number of distinct values.

    &#34;&#34;&#34;
    n = mpmath.mpf(N)
    p = mpmath.mpf(k)
    temp = mpmath.power((n - 1) / n, p)
    if do_round:
        return max(int(round(n * (1 - temp))), 0)
    else:
        return max(float(n * (1 - temp)), 0)</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.get_dist_ci"><code class="name flex">
<span>def <span class="ident">get_dist_ci</span></span>(<span>dist)</span>
</code></dt>
<dd>
<div class="desc"><p>Return 95% confidence interval.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dist_ci(dist):
    &#34;&#34;&#34;Return 95% confidence interval.&#34;&#34;&#34;
    mean = get_dist_mean(dist)
    var = get_dist_var(dist)

    lower = mean - 1.96 * sqrt(var)
    upper = mean + 1.96 * sqrt(var)

    return lower, upper</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.get_dist_mean"><code class="name flex">
<span>def <span class="ident">get_dist_mean</span></span>(<span>dist)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the expected value of a distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dist_mean(dist):
    &#34;&#34;&#34;Get the expected value of a distribution.&#34;&#34;&#34;
    mean = 0
    for k, v in dist.items():
        mean += k * v

    return mean</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.get_dist_var"><code class="name flex">
<span>def <span class="ident">get_dist_var</span></span>(<span>dist)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the variance of a distribution (OrderedDict).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dist_var(dist):
    &#34;&#34;&#34;Get the variance of a distribution (OrderedDict).&#34;&#34;&#34;
    mean = get_dist_mean(dist)

    var = 0
    for k, v in dist.items():
        to_add = v * (k - mean) * (k - mean)
        var += to_add

    return var</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.get_uniform_moments"><code class="name flex">
<span>def <span class="ident">get_uniform_moments</span></span>(<span>min_val, max_val)</span>
</code></dt>
<dd>
<div class="desc"><p>Get mean, var of a uniform distribution.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_uniform_moments(min_val, max_val):
    &#34;&#34;&#34;Get mean, var of a uniform distribution.&#34;&#34;&#34;
    mean = (max_val + min_val) / 2
    var = (1 / 12) * (max_val - min_val) * (max_val - min_val)

    return mean, var</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.hypergeometric_pmf"><code class="name flex">
<span>def <span class="ident">hypergeometric_pmf</span></span>(<span>N, K, n, k, approx=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculates the hypergeometric probability mass function.</p>
<p>See <a href="https://en.wikipedia.org/wiki/Hypergeometric_distribution">https://en.wikipedia.org/wiki/Hypergeometric_distribution</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>N</code></strong> :&ensp;<code>float</code></dt>
<dd>The population size.</dd>
<dt><strong><code>K</code></strong> :&ensp;<code>float</code></dt>
<dd>The number of success states in the population.</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of draws.</dd>
<dt><strong><code>k</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of observed successes.</dd>
<dt><strong><code>approx</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, approximate the result by the binomial distribution.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>The pmf pX(k).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hypergeometric_pmf(N, K, n, k, approx=False):
    &#34;&#34;&#34;
    Calculates the hypergeometric probability mass function.

    See https://en.wikipedia.org/wiki/Hypergeometric_distribution

    Parameters
    ----------
    N : float
        The population size.
    K : float
        The number of success states in the population.
    n : int
        The number of draws.
    k : int
        The number of observed successes.
    approx : bool
        If True, approximate the result by the binomial distribution.

    Returns
    -------
    float
        The pmf pX(k).

    &#34;&#34;&#34;
    if approx is True:
        proportion_true = K / N
        return binomial_pmf(n, k, proportion_true)

    left = C(K, k)
    right = C(N - K, n - k)
    btm = C(N, n)
    return left * right / btm</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.interp"><code class="name flex">
<span>def <span class="ident">interp</span></span>(<span>x_samps, xvals, yvals)</span>
</code></dt>
<dd>
<div class="desc"><p>Interpolate linearly between xvals and yvals at x_samps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def interp(x_samps, xvals, yvals):
    &#34;&#34;&#34;Interpolate linearly between xvals and yvals at x_samps.&#34;&#34;&#34;
    if not np.all(np.diff(xvals) &gt; 0):
        raise ValueError(&#34;Require monotonically increasing x values to interp&#34;)
    try:
        if len(xvals) == 1:
            yinterp = yvals
        kind = &#34;linear&#34;
        f = interpolate.interp1d(xvals, yvals, kind=kind)
        yinterp = f(x_samps)
    except BaseException:
        yinterp = np.interp(x_samps, xvals, yvals)

    return yinterp</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.marginal_prob"><code class="name flex">
<span>def <span class="ident">marginal_prob</span></span>(<span>eval_range, dist, fn_to_eval, x=None, sub=None, plot=False, true_plot=False, keep_all=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Marginal probability of dist over eval_range calculating fn.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def marginal_prob(
    eval_range,
    dist,
    fn_to_eval,
    x=None,
    sub=None,
    plot=False,
    true_plot=False,
    keep_all=False,
):
    &#34;&#34;&#34;Marginal probability of dist over eval_range calculating fn.&#34;&#34;&#34;
    final_res = OrderedDict()

    def x_prob(x_val):
        if sub is not None:
            in_sub = sub / 10
        else:
            in_sub = sub

        keys, did_sub = subsample_list(dist.keys(), in_sub)
        weighted_prob = np.zeros(shape=len(keys), dtype=np.float64)
        for i, k in enumerate(keys):
            v = dist[k]
            if not isclose(v, 0.0, abs_tol=1e-8):
                weighted_prob[i] = v * fn_to_eval(k, x_val)

        if did_sub:
            x_samps = list(dist.keys())
            xvals = keys
            yvals = weighted_prob
            xvals, yvals = second_deriv_improve_interp(
                x_samps, xvals, yvals, dist, fn_to_eval, x_val
            )
            weighted_prob = interp(x_samps, xvals, yvals)
            weighted_prob[weighted_prob &lt; 0] = 0.0

            if plot is True:
                if true_plot is True:
                    keys, did_sub = subsample_list(dist.keys(), None)
                    true_y = np.zeros(shape=len(keys), dtype=np.float64)
                    for i, k in enumerate(keys):
                        v = dist[k]
                        true_y[i] = v * fn_to_eval(k, x_val)
                else:
                    true_y = None
                plot_acc_interp(
                    x_samps,
                    weighted_prob,
                    xvals,
                    yvals,
                    &#34;dist_marg_{}.pdf&#34;.format(x_val),
                    true_y=true_y,
                )

        if keep_all:
            od = OrderedDict()
            for k, v in zip(dist.keys(), weighted_prob):
                od[k] = v
            return od
        else:
            return np.sum(weighted_prob)

    if x is None:
        for i in eval_range:
            final_res[i] = x_prob(i)
        if keep_all:
            return final_res

        final_res_vals = np.array(list(final_res.values()))
        pdf_total = np.sum(final_res_vals)
        if sub is not None:
            for k, v in final_res.items():
                final_res[k] = v / pdf_total
        else:
            if abs(pdf_total - 1.0) &gt; 0.0001:
                raise RuntimeError(
                    &#34;PDF does not sum to 1, got {} which sums to {}.&#34;.format(
                        final_res_vals, pdf_total
                    )
                )
        return final_res
    else:
        if sub is not None:
            raise ValueError(&#34;x None and sub None together not compatible.&#34;)
        return x_prob(x)</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.multi_2d_avg"><code class="name flex">
<span>def <span class="ident">multi_2d_avg</span></span>(<span>AB, BA, AA, BB, k_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Combine separate into one sparse matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multi_2d_avg(AB, BA, AA, BB, k_size):
    &#34;&#34;&#34;Combine separate into one sparse matrix.&#34;&#34;&#34;
    x, y = AB.shape

    a_size = int(ceil(x / k_size))
    b_size = int(ceil(y / k_size))

    full_mat = np.zeros(shape=(a_size + b_size, a_size + b_size), dtype=np.float32)
    full_mat[:a_size, :a_size] = window_2d_avg(AA, k_size)
    full_mat[:a_size, a_size:] = window_2d_avg(AB, k_size)
    full_mat[a_size:, :a_size] = window_2d_avg(BA, k_size)
    full_mat[a_size:, a_size:] = window_2d_avg(BB, k_size)

    return full_mat</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.nfold_conv"><code class="name flex">
<span>def <span class="ident">nfold_conv</span></span>(<span>args, sub=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Computes SUM(args) where args are distributions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def nfold_conv(args, sub=None):
    &#34;&#34;&#34;Computes SUM(args) where args are distributions.&#34;&#34;&#34;
    if len(args) &gt;= 2:
        conv = convolution(args[0], args[1], sub=sub)
        for arg in args[2:]:
            conv = convolution(conv, arg, sub=sub)
        return conv
    else:
        return args[0]</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.random_draw_dist"><code class="name flex">
<span>def <span class="ident">random_draw_dist</span></span>(<span>max_samples, dist, total, apply_fn=True, keep_all=False, clt_start=30, sub=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Expected unique values with n_samples from dist from total possible.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_draw_dist(
    max_samples, dist, total, apply_fn=True, keep_all=False, clt_start=30, sub=None
):
    &#34;&#34;&#34;Expected unique values with n_samples from dist from total possible.&#34;&#34;&#34;

    def fn_to_apply(k):
        return float(expected_unique(total, k))

    sum_dist = None
    mean, var = get_dist_mean(dist), get_dist_var(dist)
    results = OrderedDict()
    for n_samples in range(0, int(max_samples) + 1):
        if n_samples == 0:
            if keep_all:
                to_out = OrderedDict()
                to_out[0] = 1
                results[n_samples] = to_out
            else:
                results[n_samples] = 0
            continue
        elif n_samples &lt;= 2:
            dists_to_conv = [dist] * int(n_samples)
        else:
            dists_to_conv = [sum_dist, dist]
        if n_samples &lt; clt_start:
            sum_dist = nfold_conv(dists_to_conv, sub=sub)
        else:
            max_val = max(list(dist.keys()))
            sum_dist = create_normal(
                range((int(max_val * n_samples)) + 1),
                mean * n_samples,
                var * n_samples,
                sub=sub,
                interp_after=True,
            )
        if apply_fn is False:
            results[n_samples] = sum_dist
            continue
        fn_dist = apply_fn_to_dist(sum_dist, fn_to_apply)
        if keep_all:
            results[n_samples] = fn_dist
        else:
            results[n_samples] = get_dist_mean(fn_dist)

    return results</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.second_deriv_improve_interp"><code class="name flex">
<span>def <span class="ident">second_deriv_improve_interp</span></span>(<span>x_samps, xvals, yvals, dist, fn_to_eval, x_val)</span>
</code></dt>
<dd>
<div class="desc"><p>Given samples at xvals and yvals, second derivative to detect change points.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def second_deriv_improve_interp(x_samps, xvals, yvals, dist, fn_to_eval, x_val):
    &#34;&#34;&#34;Given samples at xvals and yvals, second derivative to detect change points.&#34;&#34;&#34;
    grad = np.gradient(yvals)
    diffs = np.abs(grad)
    grad_2 = np.gradient(grad)

    m = np.mean(diffs)
    s = np.std(diffs)
    ub = m + (3 * s)
    outliers = []
    for i, val in enumerate(diffs[:2]):
        if val &gt;= ub:
            outliers.append(i)
            outliers.append(i + 1)
    for i, val in enumerate(diffs[-2:]):
        if val &gt;= ub:
            outliers.append(i)
            outliers.append(i + 1)

    for i in range(len(grad_2) - 1):
        tc = abs(grad_2[i + 1] - grad_2[i])
        cv = np.abs((grad_2[i + 1] + grad_2[i]))
        if tc &gt; cv and tc &gt; (0.00000000001):
            outliers.append(i)

    outliers = sorted(list(set(outliers)))

    total_so_far = 0
    for val in outliers:
        start_idx = total_so_far + val
        new_x = np.array(
            list(range(xvals[start_idx] + 1, xvals[start_idx + 1])),
            dtype=np.int32,
        )
        new_y = np.zeros(
            shape=(len(new_x)),
            dtype=np.float64,
        )
        for i_idx, i in enumerate(new_x):
            v = dist[i]
            if not isclose(v, 0.0, abs_tol=1e-8):
                new_y[i_idx] = v * fn_to_eval(i, x_val)
            else:
                new_y[i_idx] = 0.0

        first_bit_x, second_bit_x = (
            xvals[: start_idx + 1],
            xvals[start_idx + 1 :],
        )
        first_bit_y, second_bit_y = (
            yvals[: start_idx + 1],
            yvals[start_idx + 1 :],
        )
        total_so_far += len(new_x)
        xvals = np.concatenate((first_bit_x, new_x, second_bit_x))
        yvals = np.concatenate((first_bit_y, new_y, second_bit_y))

    return xvals, yvals</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.subsample_list"><code class="name flex">
<span>def <span class="ident">subsample_list</span></span>(<span>list_to_sub, sub_rate)</span>
</code></dt>
<dd>
<div class="desc"><p>Subsample a list that is assumed to have linearly spaced values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subsample_list(list_to_sub, sub_rate):
    &#34;&#34;&#34;Subsample a list that is assumed to have linearly spaced values.&#34;&#34;&#34;
    to_eval_temp = list(list_to_sub)
    if sub_rate is not None:
        min_ev = min(to_eval_temp)
        max_ev = max(to_eval_temp)
        step = int(floor(max_ev - min_ev) * sub_rate)
        if step == 0:
            return to_eval_temp, False
        else:
            to_eval = list(range(min_ev, max_ev, step))
        if max_ev not in to_eval:
            to_eval.append(max_ev)
        return to_eval, True
    else:
        return to_eval_temp, False</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.trim_zeros"><code class="name flex">
<span>def <span class="ident">trim_zeros</span></span>(<span>dist)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trim_zeros(dist):
    new_dist = OrderedDict()

    values = []
    for k, v in dist.items():
        if v != 0.0:
            values.append(k)

    if len(values) &gt; 1:
        min_val = min(values)
        max_val = max(values)

        for i in range(min_val, max_val + 1):
            new_dist[i] = dist.get(i, 0)
    else:
        new_dist[values[0]] = dist.get(values[0], 0)

    return new_dist</code></pre>
</details>
</dd>
<dt id="neuroconnect.connect_math.window_2d_avg"><code class="name flex">
<span>def <span class="ident">window_2d_avg</span></span>(<span>matrix, k_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the average of matrix in k_size * k_size windows.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def window_2d_avg(matrix, k_size):
    &#34;&#34;&#34;Get the average of matrix in k_size * k_size windows.&#34;&#34;&#34;
    x, y = matrix.shape
    add_val = 1 / (k_size * k_size)

    new_x = int(ceil(x / k_size))
    new_y = int(ceil(y / k_size))

    out = np.zeros(shape=(new_x, new_y), dtype=np.float32)

    nonzeros_x, nonzeros_y, _ = sparse.find(matrix)
    for i, j in zip(nonzeros_x, nonzeros_y):
        x_idx = int(floor(i / k_size))
        y_idx = int(floor(j / k_size))
        out[x_idx, y_idx] += add_val

    # for i in range(new_x):
    #     for j in range(new_y):
    #         out[i, j] = np.mean(
    #             matrix[i * k_size : (i + 1) * k_size, j * k_size : (j + 1) * k_size]
    #         )

    return out</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neuroconnect" href="index.html">neuroconnect</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="neuroconnect.connect_math.C" href="#neuroconnect.connect_math.C">C</a></code></li>
<li><code><a title="neuroconnect.connect_math.alt_way" href="#neuroconnect.connect_math.alt_way">alt_way</a></code></li>
<li><code><a title="neuroconnect.connect_math.apply_fn_to_dist" href="#neuroconnect.connect_math.apply_fn_to_dist">apply_fn_to_dist</a></code></li>
<li><code><a title="neuroconnect.connect_math.binomial_pmf" href="#neuroconnect.connect_math.binomial_pmf">binomial_pmf</a></code></li>
<li><code><a title="neuroconnect.connect_math.combine_dists" href="#neuroconnect.connect_math.combine_dists">combine_dists</a></code></li>
<li><code><a title="neuroconnect.connect_math.convolution" href="#neuroconnect.connect_math.convolution">convolution</a></code></li>
<li><code><a title="neuroconnect.connect_math.create_normal" href="#neuroconnect.connect_math.create_normal">create_normal</a></code></li>
<li><code><a title="neuroconnect.connect_math.create_uniform" href="#neuroconnect.connect_math.create_uniform">create_uniform</a></code></li>
<li><code><a title="neuroconnect.connect_math.dist_from_file" href="#neuroconnect.connect_math.dist_from_file">dist_from_file</a></code></li>
<li><code><a title="neuroconnect.connect_math.dist_from_samples" href="#neuroconnect.connect_math.dist_from_samples">dist_from_samples</a></code></li>
<li><code><a title="neuroconnect.connect_math.expected_non_overlapping" href="#neuroconnect.connect_math.expected_non_overlapping">expected_non_overlapping</a></code></li>
<li><code><a title="neuroconnect.connect_math.expected_overlapping" href="#neuroconnect.connect_math.expected_overlapping">expected_overlapping</a></code></li>
<li><code><a title="neuroconnect.connect_math.expected_unique" href="#neuroconnect.connect_math.expected_unique">expected_unique</a></code></li>
<li><code><a title="neuroconnect.connect_math.get_dist_ci" href="#neuroconnect.connect_math.get_dist_ci">get_dist_ci</a></code></li>
<li><code><a title="neuroconnect.connect_math.get_dist_mean" href="#neuroconnect.connect_math.get_dist_mean">get_dist_mean</a></code></li>
<li><code><a title="neuroconnect.connect_math.get_dist_var" href="#neuroconnect.connect_math.get_dist_var">get_dist_var</a></code></li>
<li><code><a title="neuroconnect.connect_math.get_uniform_moments" href="#neuroconnect.connect_math.get_uniform_moments">get_uniform_moments</a></code></li>
<li><code><a title="neuroconnect.connect_math.hypergeometric_pmf" href="#neuroconnect.connect_math.hypergeometric_pmf">hypergeometric_pmf</a></code></li>
<li><code><a title="neuroconnect.connect_math.interp" href="#neuroconnect.connect_math.interp">interp</a></code></li>
<li><code><a title="neuroconnect.connect_math.marginal_prob" href="#neuroconnect.connect_math.marginal_prob">marginal_prob</a></code></li>
<li><code><a title="neuroconnect.connect_math.multi_2d_avg" href="#neuroconnect.connect_math.multi_2d_avg">multi_2d_avg</a></code></li>
<li><code><a title="neuroconnect.connect_math.nfold_conv" href="#neuroconnect.connect_math.nfold_conv">nfold_conv</a></code></li>
<li><code><a title="neuroconnect.connect_math.random_draw_dist" href="#neuroconnect.connect_math.random_draw_dist">random_draw_dist</a></code></li>
<li><code><a title="neuroconnect.connect_math.second_deriv_improve_interp" href="#neuroconnect.connect_math.second_deriv_improve_interp">second_deriv_improve_interp</a></code></li>
<li><code><a title="neuroconnect.connect_math.subsample_list" href="#neuroconnect.connect_math.subsample_list">subsample_list</a></code></li>
<li><code><a title="neuroconnect.connect_math.trim_zeros" href="#neuroconnect.connect_math.trim_zeros">trim_zeros</a></code></li>
<li><code><a title="neuroconnect.connect_math.window_2d_avg" href="#neuroconnect.connect_math.window_2d_avg">window_2d_avg</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>